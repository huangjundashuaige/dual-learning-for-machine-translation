{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['you', '-', 'I', 'to', 'the', 's', 'a', 't', 'of', 'it', 'me', 'is', 'and', \"I'\", 'your', 'You', 'that', 'in', 'are', 'this', 'be', 'on', 're', 'What', 'for', 'have', 'my', 'm', \"don'\", 'here', 'know', 'not', 'do', 'il', 'go', 'we', 'no', 'what', 'think', 'all', 'one', 'with', 'Harry', 'can', 'was', 'he', 'just', 'up', 'get', 'so', 'but', 'school', 'like', \"it'\", 'him', 'The', 've', 'about', \"It'\", 'right', 'out', 'come', 'at', \"You'\", 'Oh', 'her', 'will', 'Dobby', 'Big', 'if', \"you'\", 'brother', 'Why', 'been', 'back', 'now', 'see', 'how', 'How', 'by', 'from', 'them', 'did', 'sir', 'time', 'If', 'there', 'No', 'who', 'And', 'Potter', \"can'\", 'only', 'tell', 'Well', '--', 'our', 'going', 'too', 'they', 'Yes', 'This', 'Come', 'has', 'were', 'We', 'But', 'want', 'take', 'even', 'Do', 'never', 'really', 'look', \"didn'\", 'Hey', 'day', \"That'\", 'way', 'Are', 'when', 'am', 'well', \"What'\", 'us', \"Don'\", 'little', 'two', 'home', 'told', 'must', 'So', 'years', \"we'\", 'his', 'It', 'said', 'say', 'something', 'very', 'as', 'over', 'Please', 'would', 'good', 'had', 'down', 'could', 'why', 'Get', 'teacher', 'college', 'doing', 'make', 'should', 'meet', 'an', 'some', 'she', 'shik', 'bitch', 'A', 'first', 'Who', 'need', 'sorry', 'oh', 'Du-', 'money', 'moron', 'place', 'give', \"that'\", 'Mr', 'more', 'mean', 'stop', 'kill', 'That', 'any', \"what'\", 'into', 'got', 'then', 'every', 'Mrs', 'these', 'students', 'or', 'again', 'ever', 'When', 'course', \"let'\", 'let', 'My', \"they'\", 'girl', 'much', 'gonna', 'night', 'Alex', 'Alexander', 'Have', 'off', 'always', 'Is', 'He', 'understand', 'their', 'things', 'd', 'sure', 'Now', 'friends', 'man', 'hear', 'Mara', 'drink', 'Shit', 'happened', 'better', 'year', 'They', 'Just', 'today', 'okay', 'Go', 'call', 'away', 'please', \"won'\", 'might', 'secret', 'tree', 'future', 'family', 'thing', 'teachers', 'wrong', 'Right', 'worry', 'hit', 'hell', 'other', 'nothing', 'Lee', 'Not', 'yes', 'train', 'Professor', 'Slytherin', 'Gyeon-', 'woo', 'Emma', 'high', 'leave', 'Yeah', 'where', 'ask', 'Stop', 'those', 'All', 'someone', 'still', 'change', 'met', 'through', '#', 'days', 'Good', 'fucking', 'Then', 'Where', 'Really', \"Let'\", 'many', 'Yun-', 'ju', 'find', 'straight', 'help', 'remember', 'Look', 'Ms', 'together', 'thought', 'Ron', 'Malfoy', 'Kalen', 'Sang-', 'bastard', 'father', 'lot', 'work', 'another', 'mail', 'people', 'saying', 'name', 'after', 'made', 'car', 'keep', 'talk', 'Jo', 'new', 'easy', \"he'\", 'came', 'hard', 'before', 'ago', 'love', 'three', \"isn'\", 'heard', 'dead', 'already', 'because', 'does', 'Weasley', \"She'\", 'afraid', \"They'\", 'tonight', 'Gryffindor', 'professor', 'boss', 'Thank', 'anything', 'times', 'common', 'punk', 'In', 'gone', 'world', 'Sangchun', 'around', \"We'\", 'coming', 'looking', 'wanna', 'parents', 'bit', 'done', \"He'\", 'du', \"there'\", 'yet', 'Tell', 'may', 'stay', 'last', 'fine', 'Did', 'Can', 'life', 'put', 'maybe', 'long', 'voice', 'chamber', 'next', 'machine', 'past', 'Man', 'shit', 'sense', 'best', 'hair', 'Fuck', 'Stand', 'head', 'own', 'class', 'Huh', 'than', 'uncle', 'thank', '', 'boy', 'Foundation', 'yourself', 'fault', 'stand', 'means', 'getting', 'Of', 'She', 'guys', 'once', 'Ah', 'use', 'dear', 'ah', 'exactly', 'Hermione', 'different', 'wait', 'later', 'Watchit', 'flowers', 'Uh', 'Continues', 'Growling', 'e-', 'cafe', 'care', 'found', 'tomorrow', 'pull', 'Hello', 'lost', 'mind', 'room', 'beat', 'girls', 'die', 'Wanna', 'left', 'To', 'problem', 'gangster', '2', \"There'\", 'bring', 'See', 'huh', 'Bring', 'question', 'Excuse', 'chance', 'seen', 'ready', 'believe', 'hurt', \"haven'\", 'One', \"wasn'\", 'play', 'fight', 'friend', 'Your', 'hours', 'knew', 'Here', \"couldn'\", 'There', 'Maybe', 'called', 'true', '1', 'Shin', 'safe', 'such', 'watch', 'turn', 'write', 'control', 'house', 'wizard', 'talking', 'Alley', 'quite', \"aren'\", 'Snape', 'used', 'petrified', 'Two', 'tried', 'blood', 'answer', 'Norris', 'broke', 'wear', 'listen', 'Chuckles', 'Gasping', 'Grunting', 'Eloi', 'Boss', 'went', 'Myeongdong', 'everything', 'net', 'truth', 'stupid', 'Las', 'Vegas', 'till', 'great', 'become', 'live', 'half', 'English', 'second', 'age', 'old', 'often', 'difficult', 'looked', 'far', 'late', 'Be', 'scared', 'Ji-', 'study', 'trouble', 'student', 'everyone', 'sit', 'damn', 'Take', 'enough', 'feel', 'Okay', 'without', \"doesn'\", 'pay', 'being', 'buy', 'beads', 'Let', 'moment', 'teach', 'hey', 'follow', 'grades', 'Nothing', 'mom', 'hurts', 'bad', 'Does', 'big', 'saw', 'Stay', 'working', 'each', 'face', 'forget', 'rid', 'fun', 'book', \"wouldn'\", 'luck', 'cat', 'Hogwarts', 'happen', 'born', 'writing', 'For', 'hello', 'muggles', 'letters', 'ok', 'myself', 'introduce', 'strange', 'opened', 'heir', 'perhaps', 'guess', 'kept', 'Moaning', 'Myrtle', 'wanted', 'language', 'travel', 'foot', 'I’', 'slap', 'guy', 'supposed', 'Gangnam', 'taken', 'yesterday', 'Head', 'Internet', 'seems', 'cops', 'business', 'People', 'young', 'High', 'Gye', 'fuck', 'trust', 'morning', 'motherfucker', 'Call', 'whole', 'crazy', 'happy', 'upset', 'gangsters', 'brat', 'set', \"must'\", 'food', 'Our', 'Friends', 'mother', 'along', 'real', 'lunch', 'Damn', 'books', 'Like', 'able', 'Could', 'speak', 'also', 'free', 'Hurry', 'Wow', 'Sing', \"Who'\", 'Leave', 'passed', 'dare', 'child', 'learn', 'says', 'God', 'touch', 'Bastard', 'expect', 'ahead', 'prudent', 'Excellent', 'song', 'few', 'nice', 'serious', 'looks', 'bloody', 'club', 'magic', '_BAR_', \"she'\", 'making', 'mess', 'asked', 'himself', 'forever', 'most', 'terrible', 'wish', 'yours', 'Hi', 'George', \"could'\", 'Mom', 'hope', 'Daddy', 'Nice', 'Granger', 'hold', 'Severus', 'gather', 'Know', 'pixies', 'caught', 'having', 'Argus', 'Hogwart', 'month', 'Bludger', 'win', 'great-', 'during', 'On', 'With', \"Iet'\", 'rose', 'smoke', 'dream', 'Some', 'letter', 'Or', 'read', 'promised', 'bowler', 'Philby', 'Whirring', 'Woman', 'Engine', 'park', 'Sir', 'week', 'Ticking', 'Lunar', 'Leisure', 'Living', 'Indistinct', 'Will', 'Growls', 'Kim', 'important', 'younger', 'brothers', 'son', 'fool', 'address', 'makes', 'everyday', 'unimaginable', 'Chinese', 'open', 'end', 'anyone', 'run', 'while', 'prison', 'instead', 'private', 'rates', 'round', 'Math', 'general', 'curriculum', 'early', 'Something', 'pal', 'responsibility', 'tight', 'seeing', 'close', 'picking', 'ya', 'empty', 'seat', 'rat', 'rip', 'which', 'older', 'shoes', 'idiot', 'lucky', 'Moron', 'carton', 'cigarettes', 'dinner', 'trying', 'law', 'died', 'Give', 'graduate', 'Crazy', 'bleeding', 'forgot', 'rich', 'eat', 'try', 'learned', 'middle', 'correct', 'asking', 'aware', 'kid', 'excuse', 'Open', 'door', 'uniform', 'Jesus', 'expelled', 'inside', 'ass', 'graduation', 'months', \"hye'\", 'jerk', 'least', 'hands', 'anymore', 'parent', 'Students', 'staring', 'fired', 'earth', 'front', 'funny', 'kitchen', \"late?'\", \"'Get\", \"Aren'\", 'Sorry', 'step', 'legitimate', 'spend', 'pretty', 'soon', 'scene', \"'\", 'Little', 'grade', 'women', 'sleep', 'less', 'others', 'kidding', 'dumb', 'asshole', 'kind', 'function', 'several', 'families', 'fear', 'knowledge', 'pure', 'power', 'fast', 'possibly', 'outside', 'welcome', 'Sit', 'wizards', \"Can'\", 'line', 'sound', 'hoped', 'Better', 'drive', 'blame', 'Ginny', 'raids', 'boys', 'en', 'Percy', 'Dumbledore', 'Lockhart', 'Floo', 'powder', 'clearly', 'Hagrid', 'page', 'Draco', 'Lucius', 'mention', 'hurry', 'itself', 'eh', 'hand', 'wand', 'idea', 'both', 'rules', 'stuff', 'Mandrake', 'dangerous', 'nearly', 'Colin', 'YOU', '_BAR_and', 'worst', 'rest', 'longer', 'At', 'mudblood', 'spell', 'four', 'moving', 'act', 'wrote', 'until', 'potion', \"should'\", '3', 'words', 'Polyjuice', 'ball', 'bones', 'managed', 'full', 'details', 'show', 'disarm', 'heroine', 'Cine', 'Bupyung', 'order', 'heels', 'exchange', 'Forget', 'felt', 'boyfriend', '100th', 'prettiest', 'After', 'closer', 'date', 'busy', 'under', 'same', 'living', 'forgetting', 'meant', 'Laughing', 'walk', 'Gasps', 'ring', 'Murmuring', 'Both', 'subject', 'contact', 'com', 'Vox', 'Time', 'Shouting', 'Foreign', 'Language', 'Morlocks', 'Roaring', 'Girl', 'Myeogdong', 'charge', 'These', 'graduated', 'owe', 'Byeong-', 'hun', 'setting', 'telling', '12', 'liar', 'district', 'sort', 'Write', 'incident', 'Thanks', 'Instead', 'learning', 'institute', 'Private', 'cousin', 'goes', 'Noryangjin', 'institutes', 'diplomas', 'Korean', 'Spurt', '20', 'schools', 'competent', 'School', 'admitted', 'Also', 'Nobody', 'slicks', 'Congratulations', 'proud', 'bought', 'bag', 'Hold', \"Isn'\", 'Dong-', 'Gyeong-', 'sil', 'manners', 'quiet', \"women'\", 'sister', \"'White'\", 'annoying', \"Why'\", 'hye', 'poor', 'fights', 'brats', 'minds', 'Sure', 'kids', 'fighting', 'among', 'themselves', 'notice', 'homeroom', 'Over', 'classroom', 'begin', 'tail', 'Heard', 'delivering', 'disrespect', 'defy', 'gotta', 'thorough', 'shirt', 'Buy', 'shame', 'Shut', 'red', 'Bye', 'Lady', 'mad', 'laughing', 'treat', \"Here'\", 'favor', 'yeah', 'Jerk', 'stick', 'phone', 'May', 'rather', 'forward', 'allergy', 'famous', 'elegy', 'victim', \"school'\", 'Son', 'person', 'anyway', 'academic', 'retard', 'small', 'runs', 'hospital', 'receive', 'survive', 'Help', '5', 'Christ', 'College', 'properly', 'biggest', 'Everyone', 'Inside', 'Chun-', 'guk', 'Smile', 'sing', 'garden', 'orders', 'supplementary', 'fees', 'seats', 'scholarship', 'tempered', 'temper', 'collect', 'Fucking', 'test', \"weren'\", 'weak', \"who'\", 'pathetic', 'rage', 'against', 'quit', 'catch', 'knife', \"'Go\", 'earlier', 'As', \"ilche'\", 'blow', 'Looks', 'movie', 'punch', 'flying', \"is'\", 'action', 'Want', 'forgive', 'killing', 'deal', \"student'\", 'fair', 'changed', 'country', 'harder', '6', 'marry', 'send', \"shouldn'\", 'grown', 'nephew', 'Bitches', 'upstairs', 'Dumb', 'and-', 'dot-', 'Love', 'knows', \"'Star\", 'hook', 'backward', 'start', 'none', 'else', 'motel', 'expensive', 'collects', 'resigning', 'irrational', 'X', 'equals', 'Family', 'principal', 'Sang', 'golf', 'center', 'Gang-', 'afford', 'hundred', 'dollars', 'needing', 'seem', \"Where'\", 'Imagine', 'bury', 'role', 'angry', 'Never', 'Plus', 'trivial', 'foul', 'upon', 'Stupid', 'Move', 'clouds', 'kills', 'neck', 'Most', 'suggesting', 'Chamber', 'Hedwig', 'Petunia', 'bird', 'ha', 'messages', 'freaky', 'since', 'goodness', \"Mason'\", 'arrive', 'minute', 'shall', 'waiting', 'Dudley', 'bedroom', 'elf', 'equal', 'decent', 'almost', 'spoke', 'ill', 'serve', 'Wizardry', 'plot', 'nine', 'fix', 'belong', 'Hang', 'hearing', 'heaven', 'wonderful', 'note', 'dying', 'bars', 'window', 'summer', 'Morning', 'Nine', 'works', 'ministry', 'Dad', 'indeed', 'Errol', 'sent', 'trick', '_BAR_the', \"Harry'\", 'traveled', 'D-', 'Dia-', 'gon', 'yeh', 'yer', 'Knockturn', 'glasses', 'bye', 'worried', 'Ladies', 'gentlemen', 'bet', 'alone', 'killed', 'fearful', \"Weasley'\", 'state', 'lower', 'reason', 'clock', 'missed', 'dad', 'thanks', 'damage', 'fate', 'Headmaster', 'House', 'detention', 'Sprout', 'greenhouse', 'knock', 'pair', 'earmuffs', 'quickly', 'grasp', 'mandrake', 'pot', 'move', 'doomed', 'horrible', 'YOUR', 'S', 'dark', 'art', 'Class', 'arm', 'yourselves', 'Cornish', 'lots', 'spent', 'Quidditch', 'practice', 'team', 'permission', 'gift', 'calls', 'dirty', 'conversation', \"Malfoy'\", 'birth', 'alive', 'imagine', 'helping', 'fan', 'wonder', 'spooky', 'smell', 'spiders', 'enemies', 'ware', 'written', 'guilty', 'cure', 'attention', 'fetherfethertol', 'thousands', 'Salazar', 'legend', 'muggleborns', 'centuries', 'Crabbe', 'Goyle', 'break', 'Potion', 'allows', 'lead', 'proper', 'risky', 'juice', 'grief', 'clear', 'death', 'house-', 'treated', 'vermin', 'clothes', 'repeat', 'attack', 'picture', 'daylight', \"girl'\", 'excellent', 'master', 'cast', 'SYASlHAlS', 'playing', 'Parselmouth', 'snakes', 'snake', 'lived', 'martial', 'villain', 'rain', 'His', 'King', 'possible', 'subway', 'kiss', 'choose', 'cheek', 'blue', 'Because', 'feet', 'Catch', 'Out', 'eyes', 'daughter', 'gotten', 'burglar', 'visited', 'wasted', 'AlDS', 'Actually', 'satisfy', 'blind', 'bathroom', 'coke', 'coffee', 'given', 'introduced', \"Doesn'\", 'likes', 'prepared', 'sometimes', 'Sometimes', 'feelings', 'probably', 'preparing', 'split', 'apart', 'Thinking', 'memories', 'stories', 'loved', 'While', 'thoughts', 'bridge', 'planted', 'sad', 'service', 'England', 'answers', 'rejected', 'actually', 'ideas', 'dinosaurs', 'identical', 'hats', 'street', 'Must', 'lad', 'Molly', 'Dings', 'waste', 'bookkeeper', 'Clears', 'Throat', 'Sputtering', 'Spectacular', 'Revs', 'uh', 'Chuckling', 'shivering', 'diamond', 'Giggles', 'road', 'Hmm', 'Knocking', 'funeral', 'hiding', 'jeweler', 'faith', 'I-', 'Hissing', 'surprise', 'Panting', 'Street', 'Horse', 'Flowers', 'Snorts', 'thousand', 'Revving', 'Certain', 'restrictions', 'apply', 'Blackout', 'periods', 'availability', 'lunarleisureliving', 'latest', 'update', 'lively', 're-', 'information', 'Photonic', 'human', 'Accessing', 'application', 'writings', 'H', 'G', 'Wells', 'Hartdegen', 'Machine', 'Would', 'Wait', 'Calling', \"tor'\", 'built', 'wandering', 'Horn', 'Blowing', 'mor', 'New', \"quin'\", 'tan', 'rope', 'Speaking', 'Morlock', 'jungle', 'asleep', 'answered', 'Child', 'tas', 'happening', 'Too', 'depressing', 'above', 'below', 'Faster', 'Dripping', 'bred', 'ponytail', 'Long', 'inescapable', 'result', 'Screeching', 'Godspeed', 'Singing', 'Special', 'appearance', 'jung', 'Im', 'Chang-', 'jeong', 'Written', 'Directed', 'Yun', 'Je-', 'gyun', 'Dusabu-', 'ilche', 'Hero', 'bridgehead', 'Though', 'area', 'agree', 'finish', 'reconsider', 'modest', 'matured', 'mere', 'subordinate', 'dares', 'betray', 'Hopeless', 'Apt', '303', 'Cafe', 'dumbass', 'Whatever', \"Didn'\", 'Jongro', 'Bull', 'internet', 'ignorant', 'Seriously', 'characters', 'Myeondong', 'diploma', 'problems', \"boss'\", 'absence', 'acceptance', 'Institutes', 'applause', 'improved', 'educational', 'system', 'focused', 'overall', 'final', 'spurt', 'lowered', 'Born', '1981', 'twenties', 'enter', 'public', 'ones', 'reputation', 'contribution', 'Contribution', 'co-', 'ed', 'General', 'Enters', 'Ask', 'Repeat', 'Common', 'USA', 'pants', 'Lips', 'backpack', 'stuck', 'smeared', 'darn', 'gate', 'Fooling', 'collecting', 'Answer', 'transferred', 'alright', 'Being', 'leg', 'restroom', \"boy'\", 'sissy', 'Freak', 'straighten', 'frizzled', 'insane', 'hairstyle', 'Kyeong-', 'pads', 'Anyhow', 'sloppy', 'Only', 'brand', 'Freaks', 'geeks', 'Chain', 'smoking', 'bitches', 'Hope', 'lung', 'cancer', 'rot', 'Retard', 'pity', 'harsh', 'hoodlums', 'causing', 'giving', 'placed', 'eaten', 'Pleased', 'Settle', \"one'\", 'clean-', 'duty', 'third', 'row', 'surprised', 'clean', 'Fighting', 'Solve', 'Social', 'Tatoo-', 'ism', 'pupils', 'True', 'slicking', 'story', \"'Yes\", \"M'\", \"amm!'\", \"'You\", \"hard!'\", 'shining', 'freaking', 'Godfather', 'disregard', 'disdain', 'discount', 'bum', 'single', 'coin', 'burn', 'Search', 'Teachers', 'Lights', 'Pretty', 'Always', \"people'\", 'wealth', 'beans', 'eminent', \"chairman'\", 'mother-', 'in-', 'swing', 'Had', 'Suck', 'Later', 'Was', 'tick', 'nut', \"bastard'\", 'poser', 'Lunch', 'god', 'forgetful', 'Check', \"moron'\", 'gives', 'rice', 'cakes', 'pickles', 'allergic', 'Algeria', 'Mi-', 'ja', 'singer', 'queen', 'Came', 'llia', 'borrowed', 'credit', 'card', 'fraud', 'Verbs', 'silly', 'ordered', 'Thorough', 'scholarships', \"brother'\", 'essential', \"today'\", 'global', 'society', 'league', 'However', 'Urology', 'department', 'deficit', 'encourage', 'circumcision', 'completely', 'covered', 'Sunflower', 'Doctor', 'anesthesia', 'Seojin', 'Room', 'Salon', 'murders', 'positive', 'success', 'hallelujah', 'attend', 'profit', 'location', 'know-', 'mistake', 'morons', 'dressed', 'Suit', 'tie', 'slicked', 'plays', 'hwa', 'Ahn', 'Su-', 'ni', 'Haeng-', 'Heung-', 'Mustache', 'songs', 'congenial', 'swallowtail', 'butterfly', 'sang', 'Dumbass', 'fee', 'album', 'paid', 'conspiring', 'defiant', 'hot-', 'cooperate', 'affairs', 'pockets', 'embarrassed', 'physical', 'precious', \"How'\", 'deserve', 'Misbehave', 'misbehaving', 'dog', 'Nonsense', 'nonsense', 'husband', 'sight', 'cocky', 'matter', 'rotten', 'education', 'uproar', 'violence', 'Yang', 'jik', 'Tarzan', 'jokes', 'kick', 'disrespecting', 'slice', \"'Do\", \"off...'\", \"home...?'\", 'coincidence', 'thirsty', 'proverb', \"teacher'\", 'shadow', \"'Dusabu-\", \"'Gunsabu-\", 'Understand', 'drinking', 'alcohol', 'boat', 'Study', 'slightly', 'Pay', 'liquor', 'knees', 'begging', 'forgiveness', 'sucker-', 'technical', 'term', 'Action', 'punks', 'studying', 'gansters', 'changing', \"class'\", 'clue', 'filled', 'Bong-', 'calling', 'skating', 'thin', 'ice', 'resent', 'Sleep', 'senior', 'element', 'consists', 'plutonium', 'alloy', 'PLAY', 'Play', 'brought', 'results', 'audition', 'mailed', 'Address', 'Seoul', 'Seongbukgu', 'Donam-', 'dong', 'outta', 'ticks', 'Ignorant', 'email', 'y8090@', '@', 'stupidass', '4', 'dots', 'pop', 'Translate', 'circumcised', 'pronunciation', 'awful', 'complicate', 'boundaries', \"asshole!'\", \"Shot'\", 'Photo', 'Studio', 'Shot', \"Studio'\", 'pictures', 'models', 'grab', 'laid', 'Bang', 'Spin', 'sideways', 'satisfied', 'upside', 'ladies', 'finished', 'Lend', 'lend', \"'cause\", 'underage', 'grad', 'abroad', 'An', 'investigation', 'Parents', 'Back', 'position', 'expression', 'relation', 'variable', 'studies', 'Y', 'X-', '+1', 'relations', 'shopping', 'recently', \"nam'\", 'gang', 'Fortunately', 'identity', 'graduating', 'Straighten', 'villa', 'Mercedes-', 'Benz', 'salary', 'Exactly', 'Yeong-', \"Something'\", 'fishy', 'news', \"'Our\", \"month...'\", \"'on\", \"things.'\", \"'Teachers\", 'refusing', 'improper', \"rules...'\", 'posted', 'Department', \"Education'\", 'website', 'Board', 'Audit', 'Inspection', \"President'\", 'homepage', 'By', 'planning', 'destroy', 'loving', 'ends', 'inner', 'conflict', 'Principal', 'tells', 'raise', 'feeling', 'consequences', 'Chunman', 'fulfill', 'sympathy', 'Until', 'bright', 'involved', 'participate', 'disappoint', 'Retards', 'wasting', 'selfish', 'coward', 'influenced', 'politicians', 'illegal', 'appointive', 'risking', 'matters', 'ours', 'thoughtless', 'emotions', 'depend', 'calculations', 'added', 'tuition', 'cool', \"Shin'\", 'peaking', 'syndicate', 'agitate', \"Haven'\", 'disgraced', 'conscience', 'Cocky', 'taking', 'corrupt', 'clowns', 'bunch', 'men', 'black', 'Motherfuckers', 'broken', 'Watch', 'nam', 'simple', \"'tag'\", 'Tag', 'weapons', 'Bend', 'qualification', 'examination', 'studied', 'became', 'president', 'objecting', 'background', 'cruel', 'reality', \"'You'\", \"you?'\", 'allowed', 'besides', 'Vernin', 'warning', 'bored', 'hour', 'grateful', 'raised', 'baby', 'table', '_BAR_even', \"dudley'\", 'bed', '_BAR_purity', 'hearts', 'Pumpkin', 'Which', 'schedule', '_BAR_Petunia', 'lounge', 'graciousily', 'noise', 'pretending', 'exist', 'harm_BAR_to', 'career', 'Such', 'honor', 'understands', 'wander', 'shushi', '_BAR_I', 'offend', 'Offend', 'greatness', 'aweful', 'punish', 'serves', 'bound', '_BAR_Dobby', 'protect', 'warn', 'Witchcraft', 'plotting', 'Say', 'Ok', 'lamp', 'arrives', 'folks', 'devil', 'ruined', 'Japanese', 'golfer', 'joke', \"mustn'\", 'forgotten', \"Potter'\", 'disturbed', 'strangers', 'Rescuing', 'trunk', '---------', 'escaping', 'pigeon', 'anywhere', 'Happy', 'birthday', 'hi', 'briliant', 'beds', 'starving', 'breakfast', 'jumpa', 'annoy', 'Weasleys', 'Misuse', 'Muggle', 'Artifacts', 'Office', 'loves', 'facinating', 'lord', \"Ron'\", 'flew', 'enchanted', 'save', 'rubber', 'duck', 'post', 'fetch', \"Hogwarts'\", 'miss', 'cheap', 'speel', 'manage', '_BAR_Diagon', 'Dear', 'Diagon', 'bow', \"d'\", 'doin', \"'down\", 'Skulkin', \"'around\", 'dodgy', 'ter', '_BAR_people', 'lookin', \"'fer\", 'Flesh-', \"Eatin'\", 'Slug', 'Repellent', 'ruinin', \"'the\", 'cabbages', 'repair', 'definately', 'remmeber', 'grate', 'GlLDEROY', 'LOCKHART', 'fancies', 'madem', 'Daily', 'Prophet', 'mam', 'smile', 'extraordinary', 'bookshop', 'girlfriend', 'nicely', 'scar', 'legended', 'gave', 'Voldemort', 'murderer', 'brave', 'foolish', 'creats', 'itsself', 'Muggles', 'vacant', 'expressions', 'secondhand', 'children', 'smelling', 'Arthur', 'paying', 'overtime', '_BAR_no', 'judge', 'disgrace', 'wizard_BAR_if', 'idea_BAR_of', 'disgraces', 'associating', 'sink', '10', '58', 'leaving', 'ai', 'trolley', 'gateway', 'sealed', 'leaves', '11', 'customed', 'Invisibility', 'Booster', 'faulty', 'catching', 'behind', 'sweating', 'happenning', 'Scabbers', 'shows', 'barrier', 'platform', 'three-', 'quarters', 'lads', 'castle', '7', 'risked', 'expose', 'inflict', 'Whomping', 'Willow_BAR_which', 'ground', 'Honestly', 'Silence', 'assure', 'rested', 'McGonagall', 'flouted', 'Decree', 'Restriction', 'Underage', 'reading', 'McGonagall_BAR_to', 'determine', 'appropriate', 'expel', 'impress', 'seriousness', 'repot', 'Mandrakes', 'properties', 'root', 'Mandragora', 'return', 'original', '_BAR_Mandrake', \"cry'\", 'fatal', 'hears', 'Ten', 'points', 'mandrakes', 'seedlings', 'crys', 'protections', 'flats', 'closely', 'firmly', 'sharply', 'dump', 'pour', 'sprinky', 'soil', \"Longbottom'\", 'neglecting', 'Mam', 'fainted', 'plenty', 'pots', 'headless', 'Clearwater', 'Headless', 'Nicholas', 'Creevey', 'owl', 'howler', 'ignored', 'Gran', 'WHAT', 'WEASLEY', 'HOW', 'DARE', 'STEAL', 'THAT', 'CAR', 'AM', 'ABSOLUTELY', 'DlSGUSTlNG', 'FATHER', 'IS', 'NOW', 'FAClNG', 'AN', 'INQUlRY', 'AT', 'WORK', 'AND', \"IT'\", 'ENTlRELY', 'FAULT', 'IF', 'PUT', 'ANOTHER', 'TOE', 'OUT', 'OF', 'LlNE', \"WE'\", 'LL', 'BRlNG', 'STRAlGHT', 'HOME', 'congratulations', 'defense', 'Gilderoy', 'Order', 'Merlin', 'Third', 'Honorary', 'Member', 'Dark', 'Force', 'Defense', 'League', 'five-', 'winner', 'Witch', \"Weekly'\", 'Most-', 'Charming-', 'Award', 'Bandon', 'Banshee', 'smiling', 'warned', 'job', 'against_BAR_the', 'foulest', 'creatures', 'known', 'wizardkind', 'facing', 'fears', 'harm', 'befall', 'whilst', 'scream', 'provoke', 'Freshly', 'devilish', 'tricky', 'blighters', \"_BAR_they'\", 'Peskipiksi', 'Pesternomi', 'nip', 'cage', 'immobilas', 'devising', 'program', 'Flint', 'booked', 'patch', 'Woods', 'hereby', 'owing', 'Seeker', 'seeker', 'Nimbus', 'Thousand', 'Ones', \"Draco'\", 'unlike', 'talent', 'opinion', 'filthy', 'Mudblood', 'slugs', 'opps', 'special', 'equipment', 'curse', \"Mudblood'\", 'Muggle-', 'tongue', 'usually', 'else_BAR_because', 'pure-', 'disgusting', '_BAR_there', 'halfblood', 'point', 'sigle', \"'do\", 'harry', \"Fame'\", 'fickle', 'Celebrity', 'celebrity', '_BAR_Let', 'tear', 'lit', 'drowsy', 'Great', 'Scott', \"_BAR_We'\", 'flys', 'office', \"Filth'\", 'murdered', 'proceed', 'dormitory', 'immediately', 'except', 'Gryffindors', 'unluck', 'exact', 'countercurse', 'spare', 'wall', 'swear', 'rubbish', 'simply', 'place_BAR_at', 'however', 'circumstances', 'suspicious', 'recall', 'hungry', 'heading', 'innocent', 'proven', 'punishment', 'Madem', 'healthy', '_BAR_we', 'revive', 'strongly', 'recommend', 'caution', 'turned', '_BAR_Dumbledore', '_BAR_hearding', 'voices', 'sign', 'transforming', 'animals', 'water', 'goblets', 'fethervertol', 'replacing', 'wondering', 'founded', 'ago_BAR_by', 'greatest', 'witches', 'Godric', 'Helga', 'Hufflepuff', '_BAR_Rowena', 'Ravenclaw', 'founders', 'concensused', 'harmoniously', 'wished', 'selective', 'believed', 'magical', 'within', 'all-', 'unable', 'persuade', 'desided', 'according', 'Naturally', 'searched', 'light', 'monster', \"McGonagall'\", \"hasn'\", 'returned', 'stink', 'thinks', 'scums', 'Mudbloods', 'Heir', 'pig', 'remind', '50', 'brewed', 'drinker_BAR_to', 'transform', 'temporaly', 'phisical', 'form', 'complicated', '_BAR_he', 'attacked', 'plan', '90', '30', 'Scarhead', 'wood', 'wild', 'training', 'ballet', 'Snitch', 'arms', 'BRAKYAMY', 'IMENDO', 'mend', 'speed', 'growing', 'certainly', 'painful', 'rough', 'regrowing', 'nasty', 'bussiness', 'Pumkin', 'listened', 'stopped', 'barrier_BAR_from', 'letting', 'Indeed', 'enough_BAR_to', 'chase', 'feels', 'iron', '_BAR_or', 'strangle', 'threats', 'gets', 'five', 'suppose', 'elfs', 'mark', \"elf'\", 'enslavement', 'freed', 'masters', 'present', 'history', \"this'\", 'potter', 'Madam', 'Pomfrey', 'attacker', 'Albus', 'danger', 'staff', 'feared', 'meanever', 'Secrets', 'Chamber_BAR_when', 'brewing', 'potion_BAR_in', 'comes', 'ugly', 'miserable', 'moping', 'sensitive', 'everybody', 'lights', 'events', 'recent', 'weeks', 'granted', 'dueling', 'case', 'defend', 'countless', 'occasions', 'published', 'assistant', 'sportingly', 'agreed', 'short', 'demonstration', 'youngsters', 'Potions', 'Expelliarmus', 'cares', '_BAR_but', 'obvious', 'block', 'unfriendly', 'suggestion', 'volunteer', 'causes', 'devastation', 'simpliest', 'wing', 'match', 'box', 'suggest', 'yea', 'wands', 'count', 'charms', 'opponents', 'accidents', 'Ivertasvaty', 'Rictusempra', 'Serpensortia', 'allow', 'WOLATlOUSENDULY', 'SAYASASI', 'IPARAlAPYSKE', 'accidentally', 'boa', 'constrictor', '_BAR_on', 'zoo', 'loads', \"hadn'\", 'Justin', 'speaking', 'Parseltongue', 'realize', 'knowing', 'acted', 'signal', 'vision', 'symbol', 'serpent', '_BAR_his', 'grandson', 'film', 'bounty', 'hunter', 'wears', 'slippers', \"name'\", \"story'\", 'Japan', 'invades', \"Sejong'\", 'reign', \"king'\", 'tyrant', 'Yeonsan', 'field', 'Heavy', 'pours', 'sunny', 'Jung-', 'jo', 'Science', 'advanced', 'invent', 'UFOs', 'machines', 'Someday', 'Han', 'Suk-', 'gyu', 'Shim', 'Eun-', 'Jeon', 'Do-', 'yun', 'talked', \"Warrior'\", 'Sad', 'Story', 'sketch', 'lipstick', 'station', 'games', 'Ieft', 'prize', 'Hitting', 'Again', \"Someone'\", 'Men', 'finger', 'Saw', 'opposite', 'Left', 'Switch', 'sky', \"'Cause\", 'reflection', 'sunshine', 'seasons', 'Korea', 'word', \"'worship'\", \"'Peter\", 'prepare', \"Jesus.'\", 'Ugh', 'racket', 'teeny', 'massage', 'fit', 'sneakers', 'mine', 'carry', 'panties', 'exam', 'honey', 'Phew~', 'honey~', 'umbrella', 'wet', \"daughter'\", 'Next', 'hang', 'Iet', 'Afterwards', 'Iong', 'Her', 'sounds', 'cheerful', 'celebration', '100', 'sexy', 'woman', 'naked', 'piano', 'favorite', 'Winston', \"Pachelbel'\", \"s'\", 'Canon', 'Still', 'beaten', 'humiliated', 'broad', 'drop', \"aunt'\", 'intends', 'cleverer', 'radish', 'customer', 'relived', 'Drink', 'Gee', 'Feeling', 'either', 'suspension', 'ups', 'downs', 'tough', 'cookie', 'party', 'held', 'condom', 'prevent', 'Taxi', 'Compared', 'pain', 'cured', 'slowly', 'plans', 'pocket', \"'It'\", \"condom.'\", 'prevents', 'chicken', 'soup', 'restaurant', \"'We'\", \"wishes?'\", 'tastes', \"'Hurry\", \"him.'\", 'sudden', 'seven', \"o'\", '45', 'minutes', \"'Gotta\", \"bathroom.'\", 'Eh', 'Were', 'Coffee', 'necklace', 'ex-', 'Considering', 'pushing', 'totally', 'OK', 'obedient', 'feminine', 'aim', 'intoxicated', 'surrender', 'ten', 'recite', 'Second', 'hits', 'Make', 'fencing', 'squash', 'lightly', 'Finally', 'Encourage', 'Step', 'yellow', \"passenger'\", 'hug', 'dodge', 'definitely', 'standing', 'crossroad', 'between', 'relationship', 'paper', 'express', 'mountain', 'top', 'shout', 'helpless', 'capsule', 'good-', 'repeating', 'separate', 'Should', 'Overtime', 'endure', 'loneliness', 'started', \"future'\", 'sake', 'hitting', \"'My\", 'Sassy', \"Girl'\", 'offered', 'mystery', 'sick', 'Under', 'planned', 'suddenly', 'dating', 'prayed', 'occurred', 'seemed', 'grow', 'jealous', 'liked', 'lack', 'courage', 'angel', 'Three', 'buried', 'decision', 'Iike', 'destined', 'somewhere', 'Building', 'honest', 'struck', 'lightning', 'During', 'spring', 'number', 'dial', 'prettier', 'Feel', 'photo', 'naughty', 'resemble', 'tips', 'From', 'Think', 'coincidental', 'building', 'Bell', 'Ringing', 'Tonight', 'Almost', '00', 'proposal', 'utilizing', 'solar', 'concepts', 'harnessing', 'micro-', 'energy', 'waves', 'dean', 'suggests', 'focus', 'mankind', 'benefits', 'tobacco', 'consumption', 'radical', 'faculty', 'extinct', 'discovered', 'archaeologist', 'Their', 'dim', 'devoid', 'curiosity', 'associate', 'conservatively', 'alike', 'realities', 'greet', 'huzzah', \"master'\", 'pink', 'exercise', 'scampering', 'stairs', 'wee', 'switch', 'teeth', '40s', 'cleaned', \"shepherd'\", 'pie', 'tasted', 'hat', 'Buzzes', 'crackpot', 'interesting', 'Columbia', 'University', 'corresponding', 'German', 'patent', 'clerk', 'Einstein', 'deserves', 'support', 'Practically', 'alluring', 'chalk', 'turns', 'swooning', 'Fresh', 'Driver', 'cantilevered', 'gasket', 'fuel', 'regulator', 'Blows', 'Raspberry', 'brake', 'prior', 'engagement', 'perambulate', 'horse', 'Paris', 'picked', 'Five', \"Macy'\", 'Silly', 'distracted', 'sleeping', 'awake', 'part', 'remedy', 'entire', 'Uh-', 'moonstone', 'birthstone', 'Sighs', 'cry', 'starting', 'hate', 'moved', 'protestations', 'jewelry', 'bump', 'marital', 'bliss', 'cause', 'wallet', 'gloves', 'darling', \"Philby'\", 'insisted', 'introducing', 'Wha', 'David', 'Rapid', 'cease', 'Shall', 'city', 'Bleecker', 'quick', 'Whinnies', 'walking', 'took', 'Heavens', 'passing', 'explore', 'gadget', 'promise', 'sometime', 'Everything', 'Register', 'dozen', 'roses', 'Screams', 'Easy', 'ways', 'Down', 'Announcer', 'Distorted', 'fishing', 'fully-', 'stocked', 'Sea', 'Tranquility', 'golfing', 'championship', 'Neil', 'Armstrong', 'Fifteen-', 'hundred-', 'yard', 'drives', 'Whoa-', 'ho', 'one-', 'sixth', \"Earth'\", 'gravity', 'guaranteed', 'dance', 'floor', 'reserve', 'piece', 'suit', 'Very', 'retro', 'Bet', 'cappuccino', 'engineers', 'currently', '20-', 'megaton', 'detonation', 'create', 'subterranean', 'chambers', 'Clapping', '23', 'Antiquities', 'microscans', 'charged', 'download', 'Tommy', 'sequence', 'D', 'N', 'March', 'march', 'Fifth', 'Avenue', 'Public', 'Library', 'unit', 'registration', 'NY-', '114', 'Stereopticon', 'compendium', 'Area', 'inquiry', 'accessing', 'physics', 'Mechanical', 'engineering', 'Dimensional', 'optics', 'Chronography', 'Temporal', 'causality', 'temporal', 'paradox', 'science', 'fiction', 'Practical', 'Wh-', 'cannot', 'Isaac', 'Asimov', 'Harlan', 'Ellison', '1869', '1903', 'American', 'scientist', 'eccentric', 'postulation', 'Found', 'include', 'treatise', 'creation', '1894', 'adapted', 'motion', 'Pal', 'stage', 'musical', 'Andrew', 'Lloyd', 'Webber', 'selections', 'score', 'joy', 'sorrow', 'Radio', 'designated', 'evacuation', 'arrest', 'palms', 'rock', 'demolitions', 'lunar', 'colonies', 'screwed', 'orbit', 'Explosion', 'Shh', 'Groans', 'lex', 'stone', 'jen', 'Arguing', 'decide', 'throw', 'river', 'Getting', 'named', 'Yorkers', 'friendly', 'village', 'Boy', \"sess'\", \"Sess'\", 'Steps', 'ladder', \"lon'\", 'el', 'Tomorrow', 'tire', 'questioning', 'boats', 'shape', \"child'\", 'stones', 'places', 'taught', 'tradition', 'generation', 'passeth', 'cometh', 'abideth', 'Older', \"parents'\", \"grandparents'\", 'dwell', \"Alexander'\", 'Toren', 'N-', 'York', 'ride', 'Appears', 'group', 'Run', 'nem', \"Eloi'\", \"tasal'\", 'Fight', 'Crowd', 'Has', 'followed', 'leader', 'Listen', 'accept', 'Believe', 'ghosts', 'Animals', 'Rocks', 'Falling', 'Um', 'Henry', 'James', 'Hemingway', 'sweaty', 'Plato', 'Proust', 'Pinter', 'Poe', 'Pound', 'Complete', 'Works', 'Martha', 'Stewart', 'Jules', 'Verne', 'alley', 'T', 'Eliot', 'divine', 'lending', 'library', 'temporarily', 'volume', 'sources', 'fully', 'annotated', 'somewhat', 'anecdotal', 'race', 'distinct', 'species', 'evolved', 'avoid', 'questions', 'ambition', 'six-', 'year-', '800', '000', 'recommended', 'Homeward', 'Angel', 'Thomas', 'Wolfe', 'practical', 'escape', 'east', 'describe', 'Owls', 'Hooting', 'Heartbeat', 'Growing', 'Louder', 'toward', 'Machinery', 'Clanking', 'Snorting', 'Metallic', 'Rattling', 'Yells', 'Water', 'bite', 'escaped', 'underground', 'emerge', 'sun', 'ourselves', 'castes', 'muscles', 'sinews', 'hunters', 'Bred', 'predators', 'controlled', 'theirs', 'coldly', 'Breathing', 'Heavily', 'beings', 'perversion', 'natural', 'attempt', 'futile', 'effort', 'nightmares', 'dreams', 'haunted', 'Chattering', \"Emma'\", 'existed', 'tragedy', 'Disembodied', 'Whispering', 'Voices', 'Rumbling', 'Whooshing', 'Gagging', 'Chittering', 'Grunts', 'War', 'Cry', 'Roars', 'congregation', 'stared', 'marching', 'aisle', 'Tom', 'Joe', 'Huck', 'ruin', 'drooping', 'rags', 'Kids', 'hid', 'unused', 'gallery', 'listening', 'sermon', 'Aunt', 'Polly', 'Mary', 'Harpers', 'threw', 'laboratory', 'Gren', \"'tormar'\", 'glad', 'finally', 'someplace', 'interested', 'Perhaps', 'returns', 'changes', 'doubt', 'Hoof-', 'beats', 'Low', 'Choral', 'Tribal', 'Ends', 'Ripped', 'Divxjunkie', 'Spell', 'checking', 'corrections', 'formatting', 'alph4n53@', 'yahoo']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1962\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# coding: utf-8\n",
    "epoch=0\n",
    "\n",
    "# dependencies\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "import data_utils\n",
    "import matplotlib.pyplot as plt\n",
    "from __check import check_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y, en_word2idx, en_idx2word, en_vocab, zh_word2idx, zh_idx2word, zh_vocab = data_utils.read_dataset('en_n_zh.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1097\n"
     ]
    }
   ],
   "source": [
    "print(len(Y))\n",
    "#print(_check_how.f())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence in English - encoded: [917, 13, 6, 1452, 5, 576]\n",
      "Sentence in Chinese - encoded: [782, 8, 54, 568, 7, 1303]\n",
      "Decoded:\n",
      "------------------------\n",
      "The area is too important for him \n",
      "\n",
      "這 碼頭 對 他 很重 要 "
     ]
    }
   ],
   "source": [
    "# inspect data\n",
    "print('Sentence in English - encoded:', X[0])\n",
    "print('Sentence in Chinese - encoded:', Y[0])\n",
    "print('Decoded:\\n------------------------')\n",
    "\n",
    "for i in range(len(X[2])):\n",
    "    print(en_idx2word[X[2][i]], end=' ')\n",
    "    \n",
    "print('\\n')\n",
    "\n",
    "for i in range(len(Y[2])):\n",
    "    print(zh_idx2word[Y[2][i]], end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Singleton array array(0.5) cannot be considered a valid collection.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-276687723af1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# data processing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mX_en2zh\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY_zh2en\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY_en2zh\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_zh2en\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# data padding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdata_padding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlength\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[0;34m(*arrays, **options)\u001b[0m\n\u001b[1;32m   2029\u001b[0m         \u001b[0mtest_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.25\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2030\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2031\u001b[0;31m     \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2032\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2033\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mindexable\u001b[0;34m(*iterables)\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \"\"\"\n\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m     \u001b[0muniques\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \"\"\"\n\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m     \u001b[0muniques\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_num_samples\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m             raise TypeError(\"Singleton array %r cannot be considered\"\n\u001b[0;32m--> 119\u001b[0;31m                             \" a valid collection.\" % x)\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Singleton array array(0.5) cannot be considered a valid collection."
     ]
    }
   ],
   "source": [
    "# data processing\n",
    "X_en2zh,Y_zh2en,Y_en2zh,X_zh2en = train_test_split(X,Y,0.5)\n",
    "# data padding\n",
    "def data_padding(x, y, length = 16):\n",
    "    for i in range(len(x)):\n",
    "        x[i] = x[i] + (length - len(x[i])) * [en_word2idx['<pad>']]\n",
    "        y[i] = [zh_word2idx['<go>']] + y[i] + [zh_word2idx['<eos>']] + (length-len(y[i])) * [zh_word2idx['<pad>']]\n",
    "data_padding(X_en2zh, Y_en2zh)\n",
    "data_padding(X_zh2en, Y_zh2en)\n",
    "#print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_en2zh_train,  X_en2zh_test, Y_en2zh_train, Y_en2zh_test = train_test_split(X_en2zh, Y_en2zh, test_size = 0.1)\n",
    "X_zh2en_train, X_zh2en_test, Y_zh2en_train, Y_zh2en_test = train_test_split(X_zh2en,Y_zh2en, test_size= 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_seq_len = 16\n",
    "output_seq_len = 18\n",
    "en_vocab_size = len(en_vocab) + 2 # + <pad>, <ukn>\n",
    "zh_vocab_size = len(zh_vocab) + 4 # + <pad>, <ukn>, <eos>, <go>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(en_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_encoder_inputs = tf.placeholder(dtype = tf.int32, shape = [None,input_seq_len], name = 'encoder_en2zh')\n",
    "en_decoder_inputs = tf.placeholder(dtype = tf.int32, shape = [None,output_seq_len], name = 'decoder_en2zh')\n",
    "\n",
    "zh_encoder_inputs = tf.placeholder(dtype = tf.int32, shape = [None,input_seq_len], name = 'encoder_zh2en')\n",
    "zh_decoder_inputs = tf.placeholder(dtype = tf.int32, shape = [None,output_seq_len], name = 'decoder_zh2en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets_en2zh = tf.placeholder(dtype = tf.int32, shape = [None,output_seq_len], name = 'target_en2zh')\n",
    "# add one more target\n",
    "target_en2zh_weights = tf.placeholder(dtype = tf.int32, shape = [None,output_seq_len], name = 'target_en2zh_weights')\n",
    "targets_zh2en = tf.placeholder(dtype = tf.int32, shape = [None,output_seq_len], name = 'target_zh2en')\n",
    "# add one more target\n",
    "target_zh2en_weights = tf.placeholder(dtype = tf.int32, shape = [None,output_seq_len], name = 'target_zh2en_weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output projection\n",
    "size = 512\n",
    "w_t = tf.get_variable('proj_w', [zh_vocab_size, size], tf.float32)\n",
    "b = tf.get_variable('proj_b', [zh_vocab_size], tf.float32)\n",
    "w = tf.transpose(w_t)\n",
    "output_projection = (w, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf.reduce_mean(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"en2zh\"):\n",
    "    outputs_zh, states_zh = tf.contrib.legacy_seq2seq.embedding_attention_seq2seq(\n",
    "                                                en_encoder_inputs,\n",
    "                                                en_decoder_inputs,\n",
    "                                                tf.contrib.rnn.BasicLSTMCell(size),\n",
    "                                                num_encoder_symbols = en_vocab_size,\n",
    "                                                num_decoder_symbols = zh_vocab_size,\n",
    "                                                embedding_size = 100,\n",
    "                                                feed_previous = False,\n",
    "                                                output_projection = output_projection,\n",
    "                                                dtype = tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"zh2en\"):\n",
    "    outputs_en, states_en = tf.contrib.legacy_seq2seq.embedding_attention_seq2seq(\n",
    "                                                zh_encoder_inputs,\n",
    "                                                zh_decoder_inputs,\n",
    "                                                tf.contrib.rnn.BasicLSTMCell(size),\n",
    "                                                num_encoder_symbols = zh_vocab_size,\n",
    "                                                num_decoder_symbols = en_vocab_size,\n",
    "                                                embedding_size = 100,\n",
    "                                                feed_previous = False,\n",
    "                                                output_projection = output_projection,\n",
    "                                                dtype = tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define our loss function\n",
    "\n",
    "# sampled softmax loss - returns: A batch_size 1-D tensor of per-example sampled softmax losses\n",
    "def sampled_loss_en2zh(labels, logits):\n",
    "    return tf.nn.sampled_softmax_loss(\n",
    "                        weights = w_t,\n",
    "                        biases = b,\n",
    "                        labels = tf.reshape(labels, [-1, 1]),\n",
    "                        inputs = logits,\n",
    "                        num_sampled = 512,\n",
    "                        num_classes = zh_vocab_size)\n",
    "def sampled_loss_zh2en(labels, logits):\n",
    "    return tf.nn.sampled_softmax_loss(\n",
    "                        weights = w_t,\n",
    "                        biases = b,\n",
    "                        labels = tf.reshape(labels, [-1, 1]),\n",
    "                        inputs = logits,\n",
    "                        num_sampled = 512,\n",
    "                        num_classes = en_vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define loss using tf.cond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weighted cross-entropy loss for a sequence of logits\n",
    "loss_en2zh_warmup = tf.contrib.legacy_seq2seq.sequence_loss(outputs_zh, targets_en2zh, target_en2zh_weights, softmax_loss_function = sampled_loss_en2zh)\n",
    "\n",
    "\n",
    "loss_zh2en_warmup = tf.contrib.legacy_seq2seq.sequence_loss(outputs_en, targets_zh2en, target_zh2en_weights, softmax_loss_function = sampled_loss_zh2en)\n",
    "\n",
    "loss_odd=tf.contrib.legacy_seq2seq.sequence_loss(outputs_zh,zh_encoder_inputs , softmax_loss_function = sampled_loss_en2zh)\n",
    "loss_no_odd=tf.contrib.legacy_seq2seq.sequence_loss(outputs_en, en_encoder_inputs, softmax_loss_function = sampled_loss_en2zh)\n",
    "#lack target_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's define some helper functions\n",
    "\n",
    "# simple softmax function\n",
    "def softmax(x):\n",
    "    n = np.max(x)\n",
    "    e_x = np.exp(x - n)\n",
    "    return e_x / e_x.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feed data into placeholders\n",
    "#def feed_dict(x, y, batch_size = 64):\n",
    "def batch_generator_double(dataset1,dataset2,batch_size=50)\n",
    "    current_number=0\n",
    "    while 1:\n",
    "        if current_number+batch_size>=len(dataset1):\n",
    "            current_number=0\n",
    "        yield dataset1[current:current_number+batch_size],dataset2[current:current+batch_size]\n",
    "        current_number+=batch_size\n",
    "def batch_generator_single(dataset,batch_size=50)\n",
    "    current_number=0\n",
    "    while 1:\n",
    "        if current_number+batch_size>=len(dataset):\n",
    "            current_number=0\n",
    "        yield dataset[current:current_number+batch_size]\n",
    "        current_number+=batch_size\n",
    "def weight_calculator(batch):\n",
    "    weight=[]\n",
    "    for i in range(len(batch)):\n",
    "        if batch[i] is '<pad>':\n",
    "            weight.append(0.0)\n",
    "        else:\n",
    "            weight.append(1.0)\n",
    "    return weight\n",
    "def del_go_n_eos(batch):\n",
    "    for i in range(len(batch)):\n",
    "        if batch[i] is '<go>' or batch[i] is '<eos>':\n",
    "            del batch[i]\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    feed = {}\n",
    "    \n",
    "    idxes = np.random.choice(len(x), size = batch_size, replace = False)\n",
    "    \n",
    "    for i in range(input_seq_len):\n",
    "        feed[encoder_inputs[i].name] = np.array([x[j][i] for j in idxes], dtype = np.int32)\n",
    "        \n",
    "    for i in range(output_seq_len):\n",
    "        feed[decoder_inputs[i].name] = np.array([y[j][i] for j in idxes], dtype = np.int32)\n",
    "        \n",
    "    feed[targets[len(targets)-1].name] = np.full(shape = [batch_size], fill_value = zh_word2idx['<pad>'], dtype = np.int32)\n",
    "    \n",
    "    for i in range(output_seq_len-1):\n",
    "        batch_weights = np.ones(batch_size, dtype = np.float32)\n",
    "        target = feed[decoder_inputs[i+1].name]\n",
    "        for j in range(batch_size):\n",
    "            if target[j] == zh_word2idx['<pad>']:\n",
    "                batch_weights[j] = 0.0\n",
    "        feed[target_weights[i].name] = batch_weights\n",
    "        \n",
    "    feed[target_weights[output_seq_len-1].name] = np.zeros(batch_size, dtype = np.float32)\n",
    "    \n",
    "    return feed\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decode output sequence\n",
    "def decode_output(output_seq,idx2word=zh_id2word):\n",
    "    words = []\n",
    "    for i in range(output_seq_len):\n",
    "        smax = softmax(output_seq[i])\n",
    "        idx = np.argmax(smax)\n",
    "        words.append(idx2word[idx])\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ops and hyperparameters\n",
    "learning_rate = 5e-3\n",
    "batch_size = 64\n",
    "steps = 1000\n",
    "\n",
    "# ops for projecting outputs\n",
    "outputs_proj_en2zh = [tf.matmul(outputs_en2zh[i], output_projection[0]) + output_projection[1] for i in range(output_seq_len)]\n",
    "outputs_proj_zh2en = [tf.matmul(outputs_zh2en[i], output_projection[0]) + output_projection[1] for i in range(output_seq_len)]\n",
    "\n",
    "# training op\n",
    "with tf.name_scope(\"en2zh\"):\n",
    "    optimizer_en2zh_warmup = tf.train.RMSPropOptimizer(learning_rate).minimize(loss_en2zh_warmup)\n",
    "with tf.name_scope(\"zh2en\"):\n",
    "    optimizer_zh2en_warmup = tf.train.RMSPropOptimizer(learning_rate).minimize(loss_zh2en_warmup)\n",
    "\n",
    "\n",
    "with tf.name_scope(\"en2zh\"):\n",
    "    optimizer_odd_en2zh = tf.train.RMSPropOptimizer(learning_rate).minimize(loss_odd)\n",
    "    optimizer_no_odd_en2zh = tf.train.RMSPropOptimizer(learning_rate).minimize(loss_no_odd)\n",
    "with tf.name_scope('zh2en'):\n",
    "    optimizer_odd_zh2en = tf.train.RMSPropOptimizer(learning_rate).minimize(loss_odd)\n",
    "    optimizer_no_odd_zh2en = tf.train.RMSPropOptimizer(learning_rate).minimize(loss_no_odd)\n",
    "# init op\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# forward step\n",
    "def forward_step(sess, feed,outputs_proj):\n",
    "    output_sequences = sess.run(outputs_proj, feed_dict = feed)\n",
    "    return output_sequences\n",
    "\n",
    "# training step\n",
    "def backward_step(sess, feed,optimizer):\n",
    "    sess.run(optimizer, feed_dict = feed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_batch_en2zh=batch_generator_double(X_en2zh_train,Y_en2zh_train)\n",
    "next_bacth_zh2en=batch_generator_double(X_zh2en_train,Y_zh2en_train)\n",
    "\n",
    "next_batch_odd=batch_generator_single(X_en2zh)\n",
    "next_batch_no_odd=batch_generator_single(X_zh2en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's train the model\n",
    "n_epoch=1000\n",
    "batch_size=50\n",
    "# we will use this list to plot losses through steps\n",
    "losses = []\n",
    "\n",
    "# save a checkpoint so we can restore the model later \n",
    "saver = tf.train.Saver()\n",
    "\n",
    "print('------------------TRAINING------------------')\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    t = time.time()\n",
    "    for step in range(n_epoch/10):\n",
    "        batch_en2zh=next(next_batch_en2zh)\n",
    "        for i in range(len(batch_en2zh[1])):\n",
    "            batch_en2zh[1][i]=batch_en2zh[:len(bacth_en2zh[1][i])-2]+'<pad>'\n",
    "        batch_zh2en=next(next_batch_zh2en)\n",
    "        for i in range(len(batch_zh2en[1])):\n",
    "            batch_zh2en[1][i]=batch_zh2en[:len(bacth_zh2en[1][i])-2]+'<pad>'\n",
    "        backward_step(sess,{X_en2zh:batch_en2zh[0],target_en2zh:batch_en2zh[1]},optimizer_en2zh_warmup)\n",
    "        backward_step(sess,{X_en2zh:batch_zh2en[0],Y_zh2en:batch_zh2en[1]},optimizer_zh2en_warmup)\n",
    "        if step % 5 == 4 or step == 0:\n",
    "            loss_value_en2zh = sess.run(loss_en2zh_warmup, feed_dict = {X_en2zh_test})\n",
    "            loss_value_zh2en = sess,run(loss_zh2en_warmup, X_zh2en_test)\n",
    "            print('step: {}, loss: {}'.format(step, loss_value))\n",
    "            #losses.append(loss_value)\n",
    "        # translate\n",
    "            if step%50==0:\n",
    "                output_sequences = forward_step(sess,feed)\n",
    "                #print(output_sequences)\n",
    "                output=[]\n",
    "            # decode seq.\n",
    "                for i in range(len(output_sequences[0])):\n",
    "                    #print '{}.\\n--------------------------------'.format(i+1)\n",
    "                    ouput_seq = [output_sequences[j][i] for j in range(output_seq_len)]\n",
    "                    #decode output sequence\n",
    "                    words = decode_output(ouput_seq)\n",
    "                    word=[zh_word2idx[word] for word in words]\n",
    "                    output.append(words)\n",
    "                    #print(len(output))\n",
    "                print(output)\n",
    "                print(check_sentence(output,zh_vocab,zh_word2idx))\n",
    "        if step % 20 == 19:\n",
    "            saver.save(sess, './data/warmup')\n",
    "            print('Checkpoint is saved')\n",
    "            \n",
    "    print('Training time for {} steps: {}s'.format(steps, time.time() - t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "en_encoder_inputs = tf.placeholder(dtype = tf.int32, shape = [None,input_seq_len], name = 'encoder_en2zh')\n",
    "en_decoder_inputs = tf.placeholder(dtype = tf.int32, shape = [None,output_seq_len], name = 'decoder_en2zh')\n",
    "\n",
    "zh_encoder_inputs = tf.placeholder(dtype = tf.int32, shape = [None,input_seq_len], name = 'encoder_zh2en')\n",
    "zh_decoder_inputs = tf.placeholder(dtype = tf.int32, shape = [None,output_seq_len], name = 'decoder_zh2en')\n",
    "'''\n",
    "with tf.Session() as sess:\n",
    "        saver.restore(sess,'./data/warmup')\n",
    "        for i in range(n_epoch):\n",
    "            epoch=i\n",
    "            batch_en2zh=next(next_batch_en2zh)\n",
    "            for i in range(len(batch_en2zh[1])):\n",
    "                batch_en2zh[1][i]=batch_en2zh[:len(bacth_en2zh[1][i])-2]+'<pad>'\n",
    "            batch_zh2en=next(next_batch_zh2en)\n",
    "            for i in range(len(batch_zh2en[1])):\n",
    "                batch_zh2en[1][i]=batch_zh2en[:len(bacth_zh2en[1][i])-2]+'<pad>'\n",
    "            if epoch % 2 ==0:\n",
    "                \n",
    "                output_sequences = forward_step(sess,{X_en2zh : batch_en2zh[0]})\n",
    "                #print(output_sequences)\n",
    "                output=[]\n",
    "            # decode seq.\n",
    "                for i in range(len(output_sequences[0])):\n",
    "                    #print '{}.\\n--------------------------------'.format(i+1)\n",
    "                    ouput_seq = [output_sequences[j][i] for j in range(output_seq_len)]\n",
    "                    #decode output sequence\n",
    "                    words = decode_output(ouput_seq)\n",
    "                    #word=[zh_word2idx[word] for word in words]\n",
    "                    output.append(words)\n",
    "                    #print(len(output))\n",
    "                del_go_n_eos(output)\n",
    "                for i in range(len(batch_en2zh[0])):\n",
    "                    batch_en2zh[1][i]=batch_en2zh[:len(bacth_en2zh[0][i])-2]+'<pad>'\n",
    "                backward_step(sess,{X_en2zh:output,target_en2zh:batch_en2zh[0]},optimizer_odd_en2zh)\n",
    "                backward_step(sess,{X_en2zh:output,target_en2zh:batch_en2zh[0]},optimizer_odd_zh2en)\n",
    "            else:\n",
    "                output_sequences = forward_step(sess,{X_en2zh : batch_zh2en[0]})\n",
    "                #print(output_sequences)\n",
    "                output=[]\n",
    "            # decode seq.\n",
    "                for i in range(len(output_sequences[0])):\n",
    "                    #print '{}.\\n--------------------------------'.format(i+1)\n",
    "                    ouput_seq = [output_sequences[j][i] for j in range(output_seq_len)]\n",
    "                    #decode output sequence\n",
    "                    words = decode_output(ouput_seq)\n",
    "                    #word=[en_word2idx[word] for word in words]\n",
    "                    output.append(words)\n",
    "                    #print(len(output))\n",
    "                del_go_n_eos(output)\n",
    "                for i in range(len(batch_zh2en[0])):\n",
    "                    batch_en2zh[1][i]=batch_zh2en[:len(bacth_zh2en[0][i])-2]+'<pad>'\n",
    "                backward_step(sess,{X_zh2en:output,target_zh2en:batch_zh2en[0]},optimizer_odd_zh2en)\n",
    "                backward_step(sess,{X_zh2en:output,target_zh2en:batch_zh2en[0]},optimizer_odd_en2zh)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
